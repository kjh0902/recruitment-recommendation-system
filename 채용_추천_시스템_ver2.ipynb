{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHvtY6WY1UemDJV1A7gNX1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjh0902/recruitment-recommendation-system/blob/main/%EC%B1%84%EC%9A%A9_%EC%B6%94%EC%B2%9C_%EC%8B%9C%EC%8A%A4%ED%85%9C_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install lightfm # 하이브리드 추천 시스템을 위한 대표 라이브러리\n",
        "!pip install --upgrade gspread google-auth # 구글 시트-코랩 연동을 위한 라이브러리\n",
        "!pip install --upgrade sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C1Do7m_pUWfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f3dt8N6E-mx4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.font_manager as fm\n",
        "import warnings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import io\n",
        "import pdfplumber\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import google.auth\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"pdfminer.pdfpage\").setLevel(logging.ERROR)\n",
        "\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recruit_df = pd.read_csv(\"/content/서울시 동대문구 채용 정보.csv\", encoding='euc-kr', low_memory=False)\n",
        "\n",
        "# '제목' 컬럼에 '합격', '면접', '결과' 단어가 들어간 행을 필터링하여 제거\n",
        "recruit_df = recruit_df[~recruit_df['제목'].str.contains('합격|면접|결과', na=False)]\n",
        "recruit_df = recruit_df.drop(\"일련번호\", axis=1)\n",
        "recruit_df = recruit_df.reset_index(drop=True)\n",
        "\n",
        "recruit_df[:20]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bUSgsCIs-sEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recruit_df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UJnq0J0u_YOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link_list = recruit_df['링크'].tolist()\n",
        "link_list"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fDV36yhgUwHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요한 정보\n",
        "# 1. user_text: 유저 자신의 취업 정보와 관련된 문장\n",
        "# 2. item_text: 채용 공고 문장\n",
        "# 3. interaction_data: 유저가 관심있는 채용 공고 인덱스\n"
      ],
      "metadata": {
        "id": "kHVF_M9yHsV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hwpx_text_from_url(url):\n",
        "    # HTTP GET\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    resp = requests.get(url, headers=headers)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    # 메모리 ZIP 열기\n",
        "    hwpx_bytes = io.BytesIO(resp.content)\n",
        "    with zipfile.ZipFile(hwpx_bytes) as z:\n",
        "        # Preview 텍스트 파일 경로\n",
        "        preview_path = \"Preview/PrvText.txt\"\n",
        "        if preview_path not in z.namelist():\n",
        "            raise FileNotFoundError(f\"{preview_path} 미존재: 내부파일목록={z.namelist()}\")\n",
        "        raw = z.read(preview_path)\n",
        "        return raw.decode('utf-8')\n",
        "\n",
        "def extract_pdf_text_from_url(url):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "  response = requests.get(full_url, headers=headers)\n",
        "\n",
        "  # PDF 파일 다운로드 (메모리 상에서 처리)\n",
        "  pdf_bytes = io.BytesIO(response.content)\n",
        "\n",
        "  # pdfplumber로 텍스트 추출\n",
        "  text = \"\"\n",
        "  with pdfplumber.open(pdf_bytes) as pdf:\n",
        "      for page in pdf.pages:\n",
        "          page_text = page.extract_text()\n",
        "          if page_text:\n",
        "              text += page_text + \"\\n\"\n",
        "  return text\n",
        "\n",
        "def detect_type(url):\n",
        "    resp = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, stream=True)\n",
        "    resp.raise_for_status()\n",
        "    magic = resp.raw.read(4)\n",
        "\n",
        "\n",
        "    if magic.startswith(b\"%PDF\"):\n",
        "        return \"pdf\"\n",
        "    if magic.startswith(b\"PK\\x03\\x04\"):\n",
        "        return \"hwpx\"\n",
        "    return \"unknown\""
      ],
      "metadata": {
        "id": "T208PWWCOOH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. item_text : '링크' 열의 채용 공고 pdf 파일의 텍스트 추출\n",
        "item_texts = []\n",
        "total_links = len(link_list)\n",
        "\n",
        "for idx, i in enumerate(link_list, start=1):\n",
        "    try:\n",
        "        base_url = i  # 절대 url\n",
        "\n",
        "        # headers 추가해서 html 가져오기\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        r = requests.get(base_url, headers=headers)\n",
        "\n",
        "        # HTML 파싱\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "        # downloadBbsFile.do 가 포함된 모든 <a> 태그 중, 내부 텍스트에 \"공고\"가 포함된 태그 찾기\n",
        "        link_tag = None\n",
        "        for tag in soup.find_all('a', href=lambda x: x and 'downloadBbsFile.do' in x):\n",
        "            if '공고' in tag.get_text():\n",
        "                link_tag = tag\n",
        "                break\n",
        "\n",
        "        # 상대 주소 및 절대 URL 구성\n",
        "        relative_url = link_tag['href']\n",
        "        full_url = urljoin(base_url, relative_url)\n",
        "        file_ext = detect_type(full_url)\n",
        "        if (file_ext == \"hwpx\"):\n",
        "            text = extract_hwpx_text_from_url(full_url)\n",
        "            item_texts.append(text)\n",
        "        elif (file_ext == \"pdf\"):\n",
        "            text = extract_pdf_text_from_url(full_url)\n",
        "            item_texts.append(text)\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "    except Exception as e:\n",
        "        # 예외 발생 시, recruit_df에서 제목, 내용, 본문 텍스트 추출\n",
        "        base_url = i  # 절대 url\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        r = requests.get(base_url, headers=headers)\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "        content_element = soup.find(\"td\", class_=\"p-table__content\", title=\"내용\")\n",
        "        main_text = content_element.get_text(separator=\"\\n\", strip=True) if content_element else \"\"\n",
        "        row = recruit_df[recruit_df['링크'] == i]\n",
        "        text = str(row['제목'].iloc[0]) + \" \" + str(row['내용'].iloc[0]) + \" \" + main_text\n",
        "        item_texts.append(text)\n",
        "\n",
        "    # 10개마다 진행 상황 출력\n",
        "    if idx % 10 == 0:\n",
        "        print(f\"진행 상황: {total_links}개 중 {idx}개 처리됨.\")\n"
      ],
      "metadata": {
        "id": "pan3oHQAHrHZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# item_texts를 pickle 객체로 저장\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open(\"item_texts.ver2.pkl\", \"wb\") as f:\n",
        "    pickle.dump(item_texts, f)"
      ],
      "metadata": {
        "id": "S-z2k-I5mQ0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle 객체를 다시 item_texts로 복구\n",
        "\n",
        "import pickle\n",
        "\n",
        "# 파일을 읽기 모드로 열어서 pickle 객체를 로드합니다.\n",
        "with open('/content/item_texts.ver2.pkl', 'rb') as file:\n",
        "    item_texts = pickle.load(file)"
      ],
      "metadata": {
        "id": "mYcydoIIHHp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_texts"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gTRINMZqOd_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 전처리에 필요한 라이브러리 설치\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y openjdk-8-jdk-headless\n",
        "!pip install konlpy jpype1\n",
        "\n",
        "import re\n",
        "import pickle\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K6rSyAskQGn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 전처리 적용\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# 도메인 불용어 집합\n",
        "domain_stopwords = {\n",
        "    # 문서 메타·제목 관련\n",
        "    '서울특별시', '동대문구', '동대문구청', '동대문구청장', '동대문구인사위원회',\n",
        "    '제', '호', '공고', '재공고', '추가모집', '모집공고',\n",
        "    # 채용·근무 포맷\n",
        "    '채용', '근로자', '기간제', '기간제근로자', '사업', '참여자', '인원', '직무내용',\n",
        "    '근무기간', '근무시간', '임용', '임용분야', '담당직무', '제출서류',\n",
        "    # 일반 안내 문구\n",
        "    '붙임', '첨부', '내용', '제출', '방법', '기타', '관련', '안내', '확인', '문의',\n",
        "    # 웹·링크\n",
        "    'http', 'https', 'www', '링크'\n",
        " }\n",
        "\n",
        "def preprocess(text):\n",
        "    # 줄바꿈, nbsp 정리\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 토큰 필터링 적용\n",
        "    tokens = okt.morphs(text)\n",
        "    filtered = []\n",
        "    for t in tokens:\n",
        "        # 숫자, 짧은 거, 도메인 불용어 걸러내기\n",
        "        if t.isdigit():\n",
        "            continue\n",
        "        if len(t) <= 1:\n",
        "            continue\n",
        "        if t in domain_stopwords:\n",
        "            continue\n",
        "        filtered.append(t)\n",
        "\n",
        "\n",
        "    return \" \".join(filtered)  # 다시 문장으로 합쳐서 반환\n",
        "\n",
        "processed_texts = [preprocess(t) for t in tqdm(item_texts)]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b_GyrJDFPi75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_texts = processed_texts"
      ],
      "metadata": {
        "id": "HXvGNzwIRzB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sentence_transformers import models\n",
        "\n",
        "# 임베딩 입력 -> 실루엣 스코어를 반환\n",
        "# 응집도(cohesion)와 분리도(separation)를 종합해 −1에서 1 사이 점수로 반환(1에 가까울수록 좋음)\n",
        "def eval_emb(embs):\n",
        "    km = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "    labels = km.fit_predict(embs)\n",
        "    return silhouette_score(embs, labels)\n",
        "\n",
        "configs = ['sentence-transformers/all-MiniLM-L6-v2',\n",
        "           'jhgan/ko-sroberta-multitask',\n",
        "           'snunlp/KR-SBERT-V40K-klueNLI-augSTS',\n",
        "           'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "           'sentence-transformers/LaBSE'\n",
        "]\n",
        "\n",
        "results = []\n",
        "for c in configs:\n",
        "    print(f\"{c} 평가 시작…\")\n",
        "    tm = models.Transformer(c, max_seq_length=128)\n",
        "    pm = models.Pooling(tm.get_word_embedding_dimension(), pooling_mode='mean')\n",
        "    m  = SentenceTransformer(modules=[tm, pm])\n",
        "    embs = m.encode(item_texts, show_progress_bar=False, convert_to_numpy=True)\n",
        "    score = eval_emb(embs)\n",
        "    results.append((c, score))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Zt8oGKMggZx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(results, columns=['model', 'silhouette'])\n",
        "print(\"최종 실험 결과 (실루엣 스코어 기준 내림차순):\")\n",
        "print(df.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "w0ootcZ3qThm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.bar(df['model'], df['silhouette'])\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('silhouette scores by model')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mzJM-T_Qo1P8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. SBERT로 임베딩 (두 텍스트 모두 동일 모델 사용)\n",
        "sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# 채용 공고 텍스트 임베딩 (결과 shape: (n_items, embedding_dim))\n",
        "item_embeddings = sbert_model.encode(item_texts, normalize_embeddings=True)\n",
        "\n",
        "# 3. LightFM이 요구하는 sparse matrix 형태로 변환\n",
        "item_features = csr_matrix(item_embeddings)"
      ],
      "metadata": {
        "id": "11teZsNhKaac",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_embeddings"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5SFQW8MCkzez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_features"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kMds71M9lRQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. user_text, 3. interaction_data : 구글폼에서 입력받아서 구글 스프레드시트 가져오기\n",
        "\n",
        "# Google Sheets API를 사용할 범위 지정\n",
        "scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
        "\n",
        "# 기본 인증을 사용하는 경우\n",
        "gc = gspread.authorize(google.auth.default(scopes=scopes)[0])\n",
        "\n",
        "# 스프레드시트 열기 (문서 URL이나 제목을 사용)\n",
        "spreadsheet = gc.open(\"구직자 응답\")  # 스프레드시트 제목 사용\n",
        "worksheet = spreadsheet.sheet1  # 첫 번째 시트 선택\n",
        "\n",
        "# 데이터 가져오기\n",
        "data = worksheet.get_all_records()\n",
        "job_seekers = pd.DataFrame(data)\n",
        "\n",
        "# 구직자 프로필 텍스트 생성\n",
        "def create_profile_text(row):\n",
        "    profile = f\"직무: {job_seekers.loc[row, '직무']} 경력: {job_seekers.loc[row, '경력']} 자격증: {job_seekers.loc[row, '자격증']} 전공: {job_seekers.loc[row, '전공']} 기술: {job_seekers.loc[row, '기술']}\"\n",
        "    return profile\n",
        "\n",
        "n = 3 # 구글 시트에서 몇번째 사람 데이터인지\n",
        "user_text = create_profile_text(n)\n",
        "\n",
        "input_str = job_seekers.loc[n, '관심 있는 공고 인덱스']\n",
        "\n",
        "# 정규표현식을 사용하여 문자열에서 숫자(연속된 숫자들)를 추출합니다.\n",
        "numbers = re.findall(r'\\d+', input_str)\n",
        "\n",
        "# 추출된 문자열 숫자들을 정수로 변환하고, ('user_1', 숫자) 형태의 튜플로 리스트를 생성합니다.\n",
        "interaction_data = [('user_1', int(num)) for num in numbers]\n",
        "\n",
        "user_text"
      ],
      "metadata": {
        "id": "Tkz_JTxLFOTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. SBERT로 임베딩 (두 텍스트 모두 동일 모델 사용)\n",
        "sbert_model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
        "\n",
        "# 유저 텍스트 임베딩 (결과 shape: (1, embedding_dim))\n",
        "user_embedding = sbert_model.encode([user_text], normalize_embeddings=True)\n",
        "\n",
        "# 3. LightFM이 요구하는 sparse matrix 형태로 변환\n",
        "user_features = csr_matrix(user_embedding)\n",
        "item_features = csr_matrix(item_embeddings)\n",
        "\n",
        "# 4. Dataset 생성\n",
        "# 유저는 한 명, 아이템은 채용 공고 리스트의 인덱스로 사용\n",
        "dataset = Dataset()\n",
        "dataset.fit(users=['user_1'], items=list(range(len(item_texts))))\n",
        "\n",
        "# 5. interaction 생성\n",
        "(interactions, _) = dataset.build_interactions(interaction_data)\n",
        "\n",
        "# 6. LightFM 모델 학습 (user_features와 item_features 모두 사용)\n",
        "model = LightFM(loss='warp')\n",
        "model.fit(interactions, user_features=user_features, item_features=item_features,\n",
        "          epochs=30, num_threads=2)\n",
        "\n",
        "# 7. 유저에 대해 모든 공고 점수 예측 후 추천 순위 도출\n",
        "scores = model.predict(0, np.arange(len(item_texts)), user_features=user_features, item_features=item_features)\n",
        "recommended_order = np.argsort(-scores)\n",
        "\n",
        "print(\"<추천 결과>\")\n",
        "for idx in recommended_order[:5]:\n",
        "    print(f\"{idx}: {recruit_df.loc[idx, '제목']} (score: {scores[idx]:.4f})\")"
      ],
      "metadata": {
        "id": "r9z_kdoIBSDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKGTDM5KcvfB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}